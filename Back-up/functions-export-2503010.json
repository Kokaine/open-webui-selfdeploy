[{"id":"deepseek_api_think","user_id":"f48a35a8-e81b-4b51-9afc-96059ebf5c2a","name":"DeepSeek API Think","type":"pipe","content":"\"\"\"\ntitle: Deepseek R1 Reasoner and Chat with Realtime Thinking Preview\nauthors: Ethan Copping\nauthor_url: https://github.com/CoppingEthan\nversion: 0.3.0\nrequired_open_webui_version: 0.5.5\nlicense: MIT\n# Acknowledgments\nCode used from MCode-Team & Zgccrui\n\"\"\"\n\nimport json\nimport httpx\nfrom typing import AsyncGenerator, Callable, Awaitable\nfrom pydantic import BaseModel, Field\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        DEEPSEEK_API_BASE_URL: str = Field(\n            default=\"https://api.deepseek.com/v1\", description=\"Base API endpoint URL\"\n        )\n        DEEPSEEK_API_KEY: str = Field(\n            default=\"\", description=\"Authentication key for API access\"\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.thinking = -1\n        self._emitter = None\n        self.data_prefix = \"data: \"\n\n    def pipes(self):\n        try:\n            headers = {\"Authorization\": f\"Bearer {self.valves.DEEPSEEK_API_KEY}\"}\n            resp = httpx.get(\n                f\"{self.valves.DEEPSEEK_API_BASE_URL}/models\",\n                headers=headers,\n                timeout=10,\n            )\n            if resp.status_code == 200:\n                return [\n                    {\"id\": m[\"id\"], \"name\": m[\"id\"]}\n                    for m in resp.json().get(\"data\", [])\n                ]\n        except Exception:\n            pass\n        return [\n            {\"id\": \"deepseek-chat\", \"name\": \"deepseek-chat\"},\n            {\"id\": \"deepseek-reasoner\", \"name\": \"deepseek-reasoner\"},\n        ]\n\n    async def pipe(\n        self, body: dict, __event_emitter__: Callable[[dict], Awaitable[None]] = None\n    ) -> AsyncGenerator[str, None]:\n        self.thinking = -1\n        self._emitter = __event_emitter__\n\n        if not self.valves.DEEPSEEK_API_KEY:\n            yield json.dumps({\"error\": \"Missing API credentials\"})\n            return\n\n        req_headers = {\n            \"Authorization\": f\"Bearer {self.valves.DEEPSEEK_API_KEY}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        try:\n            request_data = body.copy()\n            model_id = request_data[\"model\"].split(\".\", 1)[-1]\n            request_data[\"model\"] = model_id\n            is_reasoner = \"reasoner\" in model_id.lower()\n\n            messages = request_data[\"messages\"]\n            for i in reversed(range(1, len(messages))):\n                if messages[i - 1][\"role\"] == messages[i][\"role\"]:\n                    alt_role = (\n                        \"user\" if messages[i][\"role\"] == \"assistant\" else \"assistant\"\n                    )\n                    messages.insert(\n                        i, {\"role\": alt_role, \"content\": \"[Unfinished thinking]\"}\n                    )\n\n            async with httpx.AsyncClient(http2=True) as client:\n                async with client.stream(\n                    \"POST\",\n                    f\"{self.valves.DEEPSEEK_API_BASE_URL}/chat/completions\",\n                    json=request_data,\n                    headers=req_headers,\n                    timeout=20,\n                ) as resp:\n                    if resp.status_code != 200:\n                        error_content = (await resp.aread()).decode()[:200]\n                        yield json.dumps(\n                            {\"error\": f\"API error {resp.status_code}: {error_content}\"}\n                        )\n                        return\n\n                    async for line in resp.aiter_lines():\n                        if not line.startswith(self.data_prefix):\n                            continue\n\n                        stream_data = json.loads(line[6:])\n                        choice = stream_data.get(\"choices\", [{}])[0]\n\n                        if choice.get(\"finish_reason\"):\n                            return\n\n                        delta = choice.get(\"delta\", {})\n\n                        if is_reasoner:\n                            state_marker = self._handle_state(delta)\n                            if state_marker:\n                                yield state_marker\n                                if state_marker == \"<think>\":\n                                    yield \"\\n\"\n\n                            content = delta.get(\"reasoning_content\", \"\") or delta.get(\n                                \"content\", \"\"\n                            )\n                            if content:\n                                yield content\n                        else:\n                            content = delta.get(\"content\", \"\")\n                            if content:\n                                yield content\n\n        except Exception as e:\n            yield json.dumps({\"error\": f\"{type(e).__name__}: {str(e)}\"})\n\n    def _handle_state(self, delta: dict) -> str:\n        if self.thinking == -1 and delta.get(\"reasoning_content\"):\n            self.thinking = 0\n            return \"<think>\"\n\n        if self.thinking == 0 and not delta.get(\"reasoning_content\"):\n            self.thinking = 1\n            return \"\\n</think>\\n\\n\"\n\n        return \"\"\n","meta":{"description":"展示DeepSeek API思考过程","manifest":{"title":"Deepseek R1 Reasoner and Chat with Realtime Thinking Preview","authors":"Ethan Copping","author_url":"https://github.com/CoppingEthan","version":"0.3.0","required_open_webui_version":"0.5.5","license":"MIT"}},"is_active":true,"is_global":false,"updated_at":1741346382,"created_at":1741070674},{"id":"tencent_think","user_id":"f48a35a8-e81b-4b51-9afc-96059ebf5c2a","name":"Tencent Think","type":"pipe","content":"\"\"\"\ntitle: DeepSeek R1\nauthor: zgccrui\ndescription: 在OpwenWebUI中显示DeepSeek R1模型的思维链 - 仅支持0.5.6及以上版本\nversion: 1.2.16\nlicence: MIT\n\"\"\"\n\nimport json\nimport httpx\nimport re\nfrom typing import AsyncGenerator, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nimport asyncio\nimport traceback\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        DEEPSEEK_API_BASE_URL: str = Field(\n            default=\"https://api.deepseek.com/v1\",\n            description=\"DeepSeek API的基础请求地址\",\n        )\n        DEEPSEEK_API_KEY: str = Field(\n            default=\"\", description=\"用于身份验证的DeepSeek API密钥，可从控制台获取\"\n        )\n        DEEPSEEK_API_MODEL: str = Field(\n            default=\"deepseek-reasoner\",\n            description=\"API请求的模型名称，默认为 deepseek-reasoner，多模型名可使用`,`分隔\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.data_prefix = \"data:\"\n        self.emitter = None\n\n    def pipes(self):\n        models = self.valves.DEEPSEEK_API_MODEL.split(\",\")\n        return [\n            {\n                \"id\": model.strip(),\n                \"name\": model.strip(),\n            }\n            for model in models\n        ]\n\n    async def pipe(\n        self, body: dict, __event_emitter__: Callable[[dict], Awaitable[None]] = None\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"主处理管道（已移除缓冲）\"\"\"\n        thinking_state = {\"thinking\": -1}  # 用于存储thinking状态\n        self.emitter = __event_emitter__\n        # 用于存储联网模式下返回的参考资料列表\n        stored_references = []\n\n        # 联网搜索供应商 0-无 1-火山引擎 2-PPLX引擎 3-硅基流动\n        search_providers = 0\n        waiting_for_reference = False\n\n        # 用于处理硅基的 [citation:1] 的栈\n        citation_stack_reference = [\n            \"[\",\n            \"c\",\n            \"i\",\n            \"t\",\n            \"a\",\n            \"t\",\n            \"i\",\n            \"o\",\n            \"n\",\n            \":\",\n            \"\",\n            \"]\",\n        ]\n        citation_stack = []\n        # 临时保存的未处理的字符串\n        unprocessed_content = \"\"\n\n        # 验证配置\n        if not self.valves.DEEPSEEK_API_KEY:\n            yield json.dumps({\"error\": \"未配置API密钥\"}, ensure_ascii=False)\n            return\n        # 准备请求参数\n        headers = {\n            \"Authorization\": f\"Bearer {self.valves.DEEPSEEK_API_KEY}\",\n            \"Content-Type\": \"application/json\",\n        }\n        try:\n            # 模型ID提取\n            model_id = body[\"model\"].split(\".\", 1)[-1]\n            payload = {**body, \"model\": model_id}\n            # 处理消息以防止连续的相同角色\n            messages = payload[\"messages\"]\n            i = 0\n            while i < len(messages) - 1:\n                if messages[i][\"role\"] == messages[i + 1][\"role\"]:\n                    # 插入具有替代角色的占位符消息\n                    alternate_role = (\n                        \"assistant\" if messages[i][\"role\"] == \"user\" else \"user\"\n                    )\n                    messages.insert(\n                        i + 1,\n                        {\"role\": alternate_role, \"content\": \"[Unfinished thinking]\"},\n                    )\n                i += 1\n\n            # 发起API请求\n            async with httpx.AsyncClient(http2=True) as client:\n                async with client.stream(\n                    \"POST\",\n                    f\"{self.valves.DEEPSEEK_API_BASE_URL}/chat/completions\",\n                    json=payload,\n                    headers=headers,\n                    timeout=300,\n                ) as response:\n\n                    # 错误处理\n                    if response.status_code != 200:\n                        error = await response.aread()\n                        yield self._format_error(response.status_code, error)\n                        return\n\n                    # 流式处理响应\n                    async for line in response.aiter_lines():\n                        if not line.startswith(self.data_prefix):\n                            continue\n\n                        # 截取 JSON 字符串\n                        json_str = line[len(self.data_prefix) :].strip()\n\n                        # 去除首尾空格后检查是否为结束标记\n                        if json_str == \"[DONE]\":\n                            return\n                        try:\n                            data = json.loads(json_str)\n                        except json.JSONDecodeError as e:\n                            error_detail = f\"解析失败 - 内容：{json_str}，原因：{e}\"\n                            yield self._format_error(\"JSONDecodeError\", error_detail)\n                            return\n\n                        if search_providers == 0:\n                            # 检查 delta 中的搜索结果\n                            choices = data.get(\"choices\")\n                            if not choices or len(choices) == 0:\n                                continue  # 跳过没有 choices 的数据块\n                            delta = choices[0].get(\"delta\", {})\n                            if delta.get(\"type\") == \"search_result\":\n                                search_results = delta.get(\"search_results\", [])\n                                if search_results:\n                                    ref_count = len(search_results)\n                                    yield '<details type=\"search\">\\n'\n                                    yield f\"<summary>已搜索 {ref_count} 个网站</summary>\\n\"\n                                    for idx, result in enumerate(search_results, 1):\n                                        yield f'> {idx}. [{result[\"title\"]}]({result[\"url\"]})\\n'\n                                    yield \"</details>\\n\"\n                                    search_providers = 3\n                                    stored_references = search_results\n                                    continue\n\n                            # 处理参考资料\n                            stored_references = data.get(\"references\", []) + data.get(\n                                \"citations\", []\n                            )\n                            if stored_references:\n                                ref_count = len(stored_references)\n                                yield '<details type=\"search\">\\n'\n                                yield f\"<summary>已搜索 {ref_count} 个网站</summary>\\n\"\n                            # 如果data中有references，则说明是火山引擎的返回结果\n                            if data.get(\"references\"):\n                                for idx, reference in enumerate(stored_references, 1):\n                                    yield f'> {idx}. [{reference[\"title\"]}]({reference[\"url\"]})\\n'\n                                yield \"</details>\\n\"\n                                search_providers = 1\n                            # 如果data中有citations，则说明是PPLX引擎的返回结果\n                            elif data.get(\"citations\"):\n                                for idx, reference in enumerate(stored_references, 1):\n                                    yield f\"> {idx}. {reference}\\n\"\n                                yield \"</details>\\n\"\n                                search_providers = 2\n\n                        # 方案 A: 检查 choices 是否存在且非空\n                        choices = data.get(\"choices\")\n                        if not choices or len(choices) == 0:\n                            continue  # 跳过没有 choices 的数据块\n                        choice = choices[0]\n\n                        # 结束条件判断\n                        if choice.get(\"finish_reason\"):\n                            return\n\n                        # 状态机处理\n                        state_output = await self._update_thinking_state(\n                            choice.get(\"delta\", {}), thinking_state\n                        )\n                        if state_output:\n                            yield state_output\n                            if state_output == \"<think>\":\n                                yield \"\\n\"\n\n                        # 处理并立即发送内容\n                        content = self._process_content(choice[\"delta\"])\n                        if content:\n                            # 处理思考状态标记\n                            if content.startswith(\"<think>\"):\n                                content = re.sub(r\"^<think>\", \"\", content)\n                                yield \"<think>\"\n                                await asyncio.sleep(0.1)\n                                yield \"\\n\"\n                            elif content.startswith(\"</think>\"):\n                                content = re.sub(r\"^</think>\", \"\", content)\n                                yield \"</think>\"\n                                await asyncio.sleep(0.1)\n                                yield \"\\n\"\n\n                            # 处理参考资料\n                            if search_providers == 1:\n                                # 火山引擎的参考资料处理\n                                # 如果文本中包含\"摘要\"，设置等待标志\n                                if \"摘要\" in content:\n                                    waiting_for_reference = True\n                                    yield content\n                                    continue\n\n                                # 如果正在等待参考资料的数字\n                                if waiting_for_reference:\n                                    # 如果内容仅包含数字或\"、\"\n                                    if re.match(r\"^(\\d+|、)$\", content.strip()):\n                                        numbers = re.findall(r\"\\d+\", content)\n                                        if numbers:\n                                            num = numbers[0]\n                                            ref_index = int(num) - 1\n                                            if 0 <= ref_index < len(stored_references):\n                                                ref_url = stored_references[ref_index][\n                                                    \"url\"\n                                                ]\n                                            else:\n                                                ref_url = \"\"\n                                            content = f\"[[{num}]]({ref_url})\"\n                                        # 保持等待状态继续处理后续数字\n                                    # 如果遇到非数字且非\"、\"的内容且不含\"摘要\"，停止等待\n                                    elif not \"摘要\" in content:\n                                        waiting_for_reference = False\n                            elif search_providers == 2:\n                                # PPLX引擎的参考资料处理\n                                def replace_ref(m):\n                                    idx = int(m.group(1)) - 1\n                                    if 0 <= idx < len(stored_references):\n                                        return f\"[[{m.group(1)}]]({stored_references[idx]})\"\n                                    return f\"[[{m.group(1)}]]()\"\n\n                                content = re.sub(r\"\\[(\\d+)\\]\", replace_ref, content)\n                            elif search_providers == 3:\n                                skip_outer = False\n\n                                if len(unprocessed_content) > 0:\n                                    content = unprocessed_content + content\n                                    unprocessed_content = \"\"\n\n                                for i in range(len(content)):\n                                    # 检查 content[i] 是否可访问\n                                    if i >= len(content):\n                                        break\n                                    # 检查 citation_stack_reference[len(citation_stack)] 是否可访问\n                                    if len(citation_stack) >= len(\n                                        citation_stack_reference\n                                    ):\n                                        break\n                                    if (\n                                        content[i]\n                                        == citation_stack_reference[len(citation_stack)]\n                                    ):\n                                        citation_stack.append(content[i])\n                                        # 如果 citation_stack 的位数等于 citation_stack_reference 的位数，则修改为 URL 格式返回\n                                        if len(citation_stack) == len(\n                                            citation_stack_reference\n                                        ):\n                                            # 检查 citation_stack[10] 是否可访问\n                                            if len(citation_stack) > 10:\n                                                ref_index = int(citation_stack[10]) - 1\n                                                # 检查 stored_references[ref_index] 是否可访问\n                                                if (\n                                                    0\n                                                    <= ref_index\n                                                    < len(stored_references)\n                                                ):\n                                                    ref_url = stored_references[\n                                                        ref_index\n                                                    ][\"url\"]\n                                                else:\n                                                    ref_url = \"\"\n\n                                                # 将content中剩余的部分保存到unprocessed_content中\n                                                unprocessed_content = \"\".join(\n                                                    content[i + 1 :]\n                                                )\n\n                                                content = f\"[[{citation_stack[10]}]]({ref_url})\"\n                                                citation_stack = []\n                                                skip_outer = False\n                                                break\n                                        else:\n                                            skip_outer = True\n                                    elif (\n                                        citation_stack_reference[len(citation_stack)]\n                                        == \"\"\n                                    ):\n                                        # 判断是否为数字\n                                        if content[i].isdigit():\n                                            citation_stack.append(content[i])\n                                            skip_outer = True\n                                        else:\n                                            # 将 citation_stack 中全部元素拼接成字符串\n                                            content = \"\".join(citation_stack) + content\n                                            citation_stack = []\n                                    elif (\n                                        citation_stack_reference[len(citation_stack)]\n                                        == \"]\"\n                                    ):\n                                        # 判断前一位是否为数字\n                                        if citation_stack[-1].isdigit():\n                                            citation_stack[-1] += content[i]\n                                            skip_outer = True\n                                        else:\n                                            content = \"\".join(citation_stack) + content\n                                            citation_stack = []\n                                    else:\n                                        if len(citation_stack) > 0:\n                                            # 将 citation_stack 中全部元素拼接成字符串\n                                            content = \"\".join(citation_stack) + content\n                                            citation_stack = []\n\n                                if skip_outer:\n                                    continue\n\n                            yield content\n        except Exception as e:\n            yield self._format_exception(e)\n\n    async def _update_thinking_state(self, delta: dict, thinking_state: dict) -> str:\n        \"\"\"更新思考状态机（简化版）\"\"\"\n        state_output = \"\"\n        if thinking_state[\"thinking\"] == -1 and delta.get(\"reasoning_content\"):\n            thinking_state[\"thinking\"] = 0\n            state_output = \"<think>\"\n        elif (\n            thinking_state[\"thinking\"] == 0\n            and not delta.get(\"reasoning_content\")\n            and delta.get(\"content\")\n        ):\n            thinking_state[\"thinking\"] = 1\n            state_output = \"\\n</think>\\n\\n\"\n        return state_output\n\n    def _process_content(self, delta: dict) -> str:\n        \"\"\"直接返回处理后的内容\"\"\"\n        return delta.get(\"reasoning_content\", \"\") or delta.get(\"content\", \"\")\n\n    def _emit_status(self, description: str, done: bool = False) -> Awaitable[None]:\n        \"\"\"发送状态更新\"\"\"\n        if self.emitter:\n            return self.emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n        return None\n\n    def _format_error(self, status_code: int, error: bytes) -> str:\n        if isinstance(error, str):\n            error_str = error\n        else:\n            error_str = error.decode(errors=\"ignore\")\n        try:\n            err_msg = json.loads(error_str).get(\"message\", error_str)[:200]\n        except Exception:\n            err_msg = error_str[:200]\n        return json.dumps(\n            {\"error\": f\"HTTP {status_code}: {err_msg}\"}, ensure_ascii=False\n        )\n\n    def _format_exception(self, e: Exception) -> str:\n        tb_lines = traceback.format_exception(type(e), e, e.__traceback__)\n        detailed_error = \"\".join(tb_lines)\n        return json.dumps({\"error\": detailed_error}, ensure_ascii=False)\n","meta":{"description":"腾讯API展示思考","manifest":{"title":"DeepSeek R1","author":"zgccrui","description":"在OpwenWebUI中显示DeepSeek R1模型的思维链 - 仅支持0.5.6及以上版本","version":"1.2.16","licence":"MIT"}},"is_active":true,"is_global":false,"updated_at":1741347768,"created_at":1741347699},{"id":"runpythoncode","user_id":"f48a35a8-e81b-4b51-9afc-96059ebf5c2a","name":"RunPythonCode","type":"action","content":"from pydantic import BaseModel, Field\nfrom typing import Optional\nfrom fastapi.requests import Request\nfrom io import StringIO\nimport sys\n\n\"\"\"\nMODERATOR COMMENT: This function should be utilized with EXTREME caution.\nDo not expose to untrusted users or deploy on secure networks unless you are sure you have considered all risks.\n\"\"\"\n\n\nclass Action:\n    class Valves(BaseModel):\n        pass\n\n    class UserValves(BaseModel):\n        show_status: bool = Field(\n            default=True, description=\"Show status of the action.\"\n        )\n        pass\n\n    def __init__(self):\n        self.valves = self.Valves()\n        pass\n\n    def execute_python_code(self, code: str) -> str:\n        \"\"\"Executes Python code and returns the output.\"\"\"\n        old_stdout = sys.stdout\n        redirected_output = sys.stdout = StringIO()\n        try:\n            exec(code, {})\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n        finally:\n            sys.stdout = old_stdout\n        return redirected_output.getvalue()\n\n    async def action(\n        self,\n        body: dict,\n        __user__=None,\n        __event_emitter__=None,\n        __event_call__=None,\n    ) -> Optional[dict]:\n        print(f\"action:{__name__}\")\n\n        user_valves = __user__.get(\"valves\")\n        if not user_valves:\n            user_valves = self.UserValves()\n\n        if __event_emitter__:\n            last_assistant_message = body[\"messages\"][-1]\n\n            if user_valves.show_status:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\"description\": \"Processing your input\", \"done\": False},\n                    }\n                )\n\n            # Execute Python code if the input is detected as code\n            input_text = last_assistant_message[\"content\"]\n            if input_text.startswith(\"```python\") and input_text.endswith(\"```\"):\n                code = input_text[9:-3].strip()  # Remove the ```python and ``` markers\n                output = self.execute_python_code(code)\n                return {\"type\": \"code_execution_result\", \"data\": {\"output\": output}}\n\n            if user_valves.show_status:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": \"No valid Python code detected\",\n                            \"done\": True,\n                        },\n                    }\n                )\n","meta":{"description":"Run Code","manifest":{}},"is_active":true,"is_global":false,"updated_at":1741416158,"created_at":1741416156},{"id":"imagegen","user_id":"f48a35a8-e81b-4b51-9afc-96059ebf5c2a","name":"智谱生图","type":"pipe","content":"\"\"\"\ntitle: DALL·E Manifold\ndescription: A manifold function to integrate OpenAI's DALL-E models into Open WebUI.\nauthor: bgeneto (based on Marc Lopez pipeline)\nfunding_url: https://github.com/open-webui\nversion: 0.1.4\nlicense: MIT\nrequirements: pydantic\nenvironment_variables: OPENAI_API_BASE_URL, OPENAI_IMG_API_KEY\n\"\"\"\n\nimport os\nfrom typing import Iterator, List, Union\n\nfrom open_webui.utils.misc import get_last_user_message\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\n\nclass Pipe:\n    \"\"\"OpenAI ImageGen pipeline\"\"\"\n\n    class Valves(BaseModel):\n\n        OPENAI_API_BASE_URL: str = Field(\n            default=\"https://api.openai.com/v1\", description=\"OpenAI API Base URL\"\n        )\n        OPENAI_IMG_API_KEY: str = Field(default=\"\", description=\"your OpenAI API key\")\n        IMAGE_SIZE: str = Field(default=\"1024x1024\", description=\"Generated image size\")\n        # NUM_IMAGES: int = Field(default=1, description=\"Number of images to generate\")\n        MODEL_NAME: str = Field(\n            default=\"cogview-4\", description=\"Your desired model name\"\n        )\n\n    def __init__(self):\n        self.type = \"manifold\"\n        self.id = \"DALL_E\"\n        self.name = \"DALL-E\"\n        self.valves = self.Valves(\n            **{\n                \"OPENAI_IMG_API_KEY\": os.getenv(\"OPENAI_IMG_API_KEY\", \"\"),\n                \"OPENAI_API_BASE_URL\": os.getenv(\n                    \"OPENAI_API_BASE_URL\", \"https://api.openai.com/v1\"\n                ),\n            }\n        )\n\n        self.client = OpenAI(\n            base_url=self.valves.OPENAI_API_BASE_URL,\n            api_key=self.valves.OPENAI_IMG_API_KEY,\n        )\n\n    # def get_openai_assistants(self) -> List[dict]:\n    #     \"\"\"Get the available ImageGen models from OpenAI\n\n    #     Returns:\n    #         List[dict]: The list of ImageGen models\n    #     \"\"\"\n\n    #     if self.valves.OPENAI_IMG_API_KEY:\n    #         self.client = OpenAI(\n    #             base_url=self.valves.OPENAI_API_BASE_URL,\n    #             api_key=self.valves.OPENAI_IMG_API_KEY,\n    #         )\n\n    #         models = self.client.models.list()\n    #         return [\n    #             {\n    #                 \"id\": model.id,\n    #                 \"name\": model.id,\n    #             }\n    #             for model in models\n    #             if \"dall-e\" in model.id\n    #         ]\n\n    #     return []\n\n    # def pipes(self) -> List[dict]:\n    #     return self.get_openai_assistants()\n\n    def pipe(self, body: dict) -> Union[str, Iterator[str]]:\n        if not self.valves.OPENAI_IMG_API_KEY:\n            return \"Error: OPENAI_IMG_API_KEY is not set\"\n\n        self.client = OpenAI(\n            base_url=self.valves.OPENAI_API_BASE_URL,\n            api_key=self.valves.OPENAI_IMG_API_KEY,\n        )\n\n        # model_id = body[\"model\"]\n        # model_id = model_id.split(\".\")[1]\n        user_message = get_last_user_message(body[\"messages\"])\n\n        response = self.client.images.generate(\n            model=self.valves.MODEL_NAME,\n            prompt=user_message,\n            size=self.valves.IMAGE_SIZE,\n            # n=self.valves.NUM_IMAGES,\n        )\n\n        message = \"\"\n        for image in response.data:\n            if image.url:\n                message += \"![image](\" + image.url + \")\\n\"\n\n        yield message\n","meta":{"description":"智谱CogView4","manifest":{"title":"DALL·E Manifold","description":"A manifold function to integrate OpenAI's DALL-E models into Open WebUI.","author":"bgeneto (based on Marc Lopez pipeline)","funding_url":"https://github.com/open-webui","version":"0.1.4","license":"MIT","requirements":"pydantic","environment_variables":"OPENAI_API_BASE_URL, OPENAI_IMG_API_KEY"}},"is_active":true,"is_global":false,"updated_at":1741617282,"created_at":1741616741},{"id":"deepseek_with_vision","user_id":"f48a35a8-e81b-4b51-9afc-96059ebf5c2a","name":"Deepseek with Vision","type":"pipe","content":"\"\"\"\ntitle: Deepseek R1 Manifold Pipe with Gemini Vision Support\nauthors: [MCode-Team, Ethan Copping, zgccrui]\nauthor_url: [https://github.com/MCode-Team, https://github.com/CoppingEthan]\nfunding_url: https://github.com/open-webui\nversion: 0.1.7\nlicense: MIT\nenvironment_variables:\n    - DEEPSEEK_API_KEY (required)\n    - GOOGLE_API_KEY (required for image processing)\n\nUser: [Text + Image]\nSystem:\n1. Gemini reads the image and generates a description.  \n2. Combines the image description with the text.  \n3. Sends the combined content to DeepSeek for processing.  \n4. DeepSeek responds back.\n\n# Acknowledgments\nAdapted code from [Ethan Copping] to add realtime preview of the thinking process for Deepseek R1\nAdapted code from [zgccrui] to add Display the reasoning chain of the DeepSeek R1\n\n\"\"\"\n\nimport os\nimport json\nimport time\nimport logging\nimport httpx\nimport re\n\n# import google.generativeai as genai\nfrom typing import (\n    List,\n    Union,\n    Generator,\n    Iterator,\n    Dict,\n    Optional,\n    AsyncIterator,\n    Tuple,\n    Awaitable,\n    Callable,\n)\nfrom pydantic import BaseModel, Field\nfrom open_webui.utils.misc import pop_system_message\nfrom openai import OpenAI\n\n\nclass CacheEntry:\n    def __init__(self, description: str):\n        self.description = description\n        self.timestamp = time.time()\n\n\nclass Pipe:\n    SUPPORTED_IMAGE_TYPES = [\"image/jpeg\", \"image/png\", \"image/gif\", \"image/webp\"]\n    MAX_IMAGE_SIZE = 5 * 1024 * 1024  # 5MB per image\n    TOTAL_MAX_IMAGE_SIZE = 100 * 1024 * 1024  # 100MB total\n    REQUEST_TIMEOUT = (3.05, 60)\n    CACHE_EXPIRATION = 30 * 60  # 30 minutes in seconds\n    MODEL_MAX_TOKENS = {\n        \"deepseek-chat\": 8192,\n        \"deepseek-reasoner\": 8192,\n        \"deepseek-r1\": 8192,\n        \"deepseek-v3\": 8192,\n    }\n\n    class Valves(BaseModel):\n        DEEPSEEK_BASE_URL: str = Field(\n            default=os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\"),\n            description=\"Your DeepSeek Base URL\",\n        )\n        DEEPSEEK_API_KEY: str = Field(\n            default=os.getenv(\"DEEPSEEK_API_KEY\", \"\"),\n            description=\"Your DeepSeek API key\",\n        )\n        DEEPSEEK_MODEL: str = Field(\n            default=os.getenv(\"DEEPSEEK_MODEL\", \"\"),\n            description=\"Your DeepSeek Model\",\n        )\n        VISION_API_KEY: str = Field(\n            default=os.getenv(\"VISION_API_KEY\", \"\"),\n            description=\"Your Vision API key for image processing\",\n        )\n        VISION_API_URL: str = Field(\n            default=os.getenv(\"VISION_API_URL\", \"\"),\n            description=\"Your Vision API URL for image processing\",\n        )\n        VISION_MODEL: str = Field(\n            default=os.getenv(\"VISION_MODEL\", \"\"),\n            description=\"Your Vision Model Name for image processing\",\n        )\n\n    def __init__(self):\n        logging.basicConfig(level=logging.INFO)\n        self.type = \"manifold\"\n        self.id = \"deepseek\"\n        self.name = \"deepseek/\"\n        self.valves = self.Valves()\n        self.request_id = None\n        self.image_cache = {}\n\n        self.clean_pattern = re.compile(r\"<details>.*?</details>\\n\\n\", flags=re.DOTALL)\n        self.buffer_size = 3\n        self.thinking_state = -1  # -1: Not started, 0: Thinking, 1: Answered\n\n    @staticmethod\n    def get_model_id(model_name: str) -> str:\n        return model_name.replace(\".\", \"/\").split(\"/\")[-1]\n\n    def get_deepseek_models(self) -> List[Dict[str, str]]:\n        try:\n            headers = {\n                \"Authorization\": f\"Bearer {self.valves.DEEPSEEK_API_KEY}\",\n                \"Content-Type\": \"application/json\",\n            }\n            with httpx.Client() as client:\n                response = client.get(\n                    f\"{self.valves.DEEPSEEK_BASE_URL}/models\",\n                    headers=headers,\n                    timeout=10,\n                )\n            response.raise_for_status()\n            models_data = response.json()\n            return [\n                {\"id\": model[\"id\"], \"name\": model[\"id\"]}\n                for model in models_data.get(\"data\", [])\n            ]\n        except Exception as e:\n            logging.error(f\"Error getting models: {e}\")\n            return []\n\n    def pipes(self) -> List[dict]:\n        return self.get_deepseek_models()\n\n    def clean_expired_cache(self):\n        current_time = time.time()\n        expired_keys = [\n            key\n            for key, entry in self.image_cache.items()\n            if current_time - entry.timestamp > self.CACHE_EXPIRATION\n        ]\n        for key in expired_keys:\n            del self.image_cache[key]\n\n    def extract_images_and_text(self, message: Dict) -> Tuple[List[Dict], str]:\n        images = []\n        text_parts = []\n        content = message.get(\"content\", \"\")\n\n        if isinstance(content, list):\n            for item in content:\n                if item.get(\"type\") == \"text\":\n                    text_parts.append(item.get(\"text\", \"\"))\n                elif item.get(\"type\") == \"image_url\":\n                    images.append(item)\n        else:\n            text_parts.append(content)\n\n        return images, \" \".join(text_parts)\n\n    async def process_image_with_gemini(\n        self, image_data: Dict, __event_emitter__=None\n    ) -> str:\n        try:\n            if not self.valves.VISION_API_KEY:\n                raise ValueError(\"VISION_API is required for image processing\")\n\n            self.clean_expired_cache()\n            image_url = image_data.get(\"image_url\", {}).get(\"url\", \"\")\n            image_key = image_url.split(\",\", 1)[1] if \",\" in image_url else image_url\n\n            if image_key in self.image_cache:\n                return self.image_cache[image_key].description\n\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": \"处理图片中...\",\n                            \"done\": False,\n                        },\n                    }\n                )\n\n            # genai.configure(api_key=self.valves.GOOGLE_API_KEY)\n            # model = genai.GenerativeModel(\"gemini-2.0-flash\")\n            client = OpenAI(\n                api_key=self.valves.VISION_API_KEY,  # 混元 APIKey\n                base_url=self.valves.VISION_API_URL,  # 混元 endpoint\n            )\n\n            # if image_url.startswith(\"data:image\"):\n            #     image_part = {\n            #         \"inline_data\": {\n            #             \"mime_type\": \"image/jpeg\",\n            #             \"data\": image_url.split(\",\", 1)[1],\n            #         }\n            #     }\n            # else:\n            #     image_part = {\"image_url\": image_url}\n            # if image_url.startswith(\"data:image\"):\n            #     file = client.files.create(\n            #         file=open(image_url.split(\",\", 1)[1], \"rb\"), purpose=\"vision\"\n            #     )\n            #     image_part = {\n            #         \"type\": \"image_file\",\n            #         \"image_file\": {\"file_id\": file.id},\n            #     }\n            # else:\n            img_url = image_url.split(\",\", 1)[1]\n            img_str = f\"data:image/jpeg;base64,{img_url}\"\n\n            # response = model.generate_content(\n            #     [\"Describe this image in detail\", image_part]\n            # )\n            response = client.chat.completions.create(\n                model=self.valves.VISION_MODEL,\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"contents\": [\n                            {\n                                \"type\": \"text\",\n                                \"text\": \"请尽可能详细的描述这张图片的全部内容\",\n                            },\n                            {\"type\": \"image_url\", \"image_url\": {\"url\": img_str}},\n                        ],\n                    },\n                ],\n            )\n            description = response.text\n\n            self.image_cache[image_key] = CacheEntry(description)\n            if len(self.image_cache) > 100:\n                oldest_key = min(\n                    self.image_cache.keys(), key=lambda k: self.image_cache[k].timestamp\n                )\n                del self.image_cache[oldest_key]\n\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": \"图片处理完成\",\n                            \"done\": True,\n                        },\n                    }\n                )\n\n            return description\n\n        except Exception as e:\n            logging.error(f\"Image processing error: {str(e)}\")\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": f\"Image Error: {str(e)}\",\n                            \"done\": True,\n                        },\n                    }\n                )\n            return f\"[Image Error: {str(e)}]\"\n\n    async def process_messages(\n        self, messages: List[Dict], __event_emitter__=None\n    ) -> List[Dict]:\n        processed_messages = []\n        for message in messages:\n            images, text = self.extract_images_and_text(message)\n            if images:\n                image_descriptions = []\n                for idx, image in enumerate(images, 1):\n                    if __event_emitter__:\n                        await __event_emitter__(\n                            {\n                                \"type\": \"status\",\n                                \"data\": {\n                                    \"description\": f\"处理照片 {idx}/{len(images)}...\",\n                                    \"done\": False,\n                                },\n                            }\n                        )\n\n                    description = await self.process_image_with_gemini(\n                        image, __event_emitter__\n                    )\n\n                    if __event_emitter__:\n                        await __event_emitter__(\n                            {\n                                \"type\": \"status\",\n                                \"data\": {\n                                    \"description\": f\"Image {idx} analysis complete\",\n                                    \"done\": True,\n                                },\n                            }\n                        )\n\n                    image_descriptions.append(f\"[Image Description: {description}]\")\n\n                combined_content = text + \" \" + \" \".join(image_descriptions)\n                processed_messages.append(\n                    {\"role\": message[\"role\"], \"content\": combined_content.strip()}\n                )\n            else:\n                processed_messages.append(message)\n        return processed_messages\n\n    async def _stream_response(\n        self,\n        url: str,\n        headers: dict,\n        payload: dict,\n        __event_emitter__=None,\n        model_id: str = \"\",\n    ) -> AsyncIterator[str]:\n        buffer = []\n        self.thinking_state = -1\n        last_status_time = time.time()\n        status_dots = 0\n\n        try:\n            async with httpx.AsyncClient() as client:\n                async with client.stream(\n                    \"POST\",\n                    url,\n                    headers=headers,\n                    json=payload,\n                    timeout=self.REQUEST_TIMEOUT,\n                ) as response:\n                    response.raise_for_status()\n\n                    async for line in response.aiter_lines():\n                        line = line.strip()\n                        if not line.startswith(\"data: \"):\n                            continue\n\n                        data_str = line[6:].strip()\n                        if data_str == \"[DONE]\":\n                            continue\n\n                        try:\n                            data = json.loads(data_str)\n                        except json.JSONDecodeError as e:\n                            logging.error(\n                                f\"Failed to parse data line: {data_str}, error: {e}\"\n                            )\n                            continue\n\n                        choice = data.get(\"choices\", [{}])[0]\n                        delta = choice.get(\"delta\", {})\n                        reasoning = delta.get(\"reasoning_content\") or \"\"\n                        content = delta.get(\"content\") or \"\"\n                        finish_reason = choice.get(\"finish_reason\")\n\n                        if self.thinking_state == -1 and reasoning:\n                            self.thinking_state = 0\n                            buffer.append(\"<details>\\n<summary>思考过程</summary>\\n\\n\")\n                            if __event_emitter__:\n                                await __event_emitter__(\n                                    {\n                                        \"type\": \"status\",\n                                        \"data\": {\n                                            \"description\": \"思考中...\",\n                                            \"done\": False,\n                                        },\n                                    }\n                                )\n\n                        elif self.thinking_state == 0 and not reasoning and content:\n                            self.thinking_state = 1\n                            buffer.append(\"\\n</details>\\n\\n\")\n                            if __event_emitter__:\n                                await __event_emitter__(\n                                    {\n                                        \"type\": \"status\",\n                                        \"data\": {\"description\": \"\", \"done\": True},\n                                    }\n                                )\n\n                        if self.thinking_state == 0 and (\n                            model_id == \"deepseek-reasoner\" or model_id == \"deepseek-r1\"\n                        ):\n                            current_time = time.time()\n                            if current_time - last_status_time > 1:\n                                status_dots = (status_dots % 3) + 1\n                                await __event_emitter__(\n                                    {\n                                        \"type\": \"status\",\n                                        \"data\": {\n                                            \"description\": f\"Thinking{'.'*status_dots}\",\n                                            \"done\": False,\n                                        },\n                                    }\n                                )\n                                last_status_time = current_time\n\n                        if reasoning:\n                            buffer.append(reasoning.replace(\"\\n\", \"\\n> \"))\n                        elif content:\n                            buffer.append(content)\n\n                        if finish_reason == \"stop\":\n                            if self.thinking_state == 0:\n                                buffer.append(\"\\n</details>\\n\\n\")\n                            break\n\n                        if len(buffer) >= self.buffer_size or \"\\n\" in (\n                            reasoning + content\n                        ):\n                            yield \"\".join(buffer)\n                            buffer.clear()\n\n                    if buffer:\n                        yield \"\".join(buffer)\n\n        except Exception as e:\n            error_msg = f\"Stream Error: {str(e)}\"\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\"type\": \"status\", \"data\": {\"description\": error_msg, \"done\": True}}\n                )\n            yield error_msg\n\n    async def _regular_request(\n        self,\n        url: str,\n        headers: dict,\n        payload: dict,\n        __event_emitter__=None,\n        model_id: str = \"\",\n    ) -> dict:\n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.post(\n                    url,\n                    headers=headers,\n                    json=payload,\n                    timeout=self.REQUEST_TIMEOUT,\n                )\n                response.raise_for_status()\n                data = response.json()\n\n                # Process DeepSeek response structure\n                if \"choices\" in data and len(data[\"choices\"]) > 0:\n                    choice = data[\"choices\"][0]\n                    message = choice.get(\"message\", {})\n                    original_content = message.get(\"content\", \"\")\n                    reasoning = message.get(\"reasoning_content\", \"\")\n\n                    # Combine reasoning and content\n                    processed_content = original_content\n                    if reasoning:\n                        processed_content = (\n                            f\"<details>\\n<summary>Thinking Process</summary>\\n\\n\"\n                            f\"{reasoning}\\n</details>\\n\\n{original_content}\"\n                        )\n                        processed_content = self.clean_pattern.sub(\n                            \"\", processed_content\n                        ).strip()\n\n                    # Modify response to match expected structure\n                    data[\"choices\"][0][\"message\"][\"content\"] = processed_content\n                    data[\"choices\"][0][\"message\"][\"reasoning_content\"] = reasoning\n\n                if __event_emitter__:\n                    await __event_emitter__(\n                        {\n                            \"type\": \"status\",\n                            \"data\": {\n                                \"description\": (\n                                    data[\"choices\"][0][\"message\"][\"content\"]\n                                    if data.get(\"choices\")\n                                    else \"\"\n                                ),\n                                \"done\": True,\n                            },\n                        }\n                    )\n\n                return data\n\n        except Exception as e:\n            error_msg = f\"Error: {str(e)}\"\n            logging.error(f\"Regular request failed: {error_msg}\")\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": error_msg,\n                            \"done\": True,\n                        },\n                    }\n                )\n            return {\"error\": error_msg, \"choices\": []}\n\n    async def pipe(\n        self, body: Dict, __event_emitter__=None\n    ) -> Union[AsyncIterator[str], dict]:\n        if not self.valves.DEEPSEEK_API_KEY:\n            error_msg = \"Error: DEEPSEEK_API_KEY is required\"\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\"type\": \"status\", \"data\": {\"description\": error_msg, \"done\": True}}\n                )\n            return {\"error\": error_msg, \"choices\": []}\n\n        try:\n            system_message, messages = pop_system_message(body.get(\"messages\", []))\n            processed_messages = await self.process_messages(\n                messages, __event_emitter__\n            )\n\n            for msg in processed_messages:\n                if msg.get(\"role\") == \"assistant\" and \"content\" in msg:\n                    msg[\"content\"] = self.clean_pattern.sub(\"\", msg[\"content\"]).strip()\n\n            model_id = self.get_model_id(body[\"model\"])\n            # model_id = self.valves.DEEPSEEK_MODEL\n            max_tokens_limit = self.MODEL_MAX_TOKENS.get(model_id, 8192)\n\n            if system_message:\n                processed_messages.insert(\n                    0, {\"role\": \"system\", \"content\": str(system_message)}\n                )\n\n            payload = {\n                \"model\": model_id,\n                \"messages\": processed_messages,\n                \"max_tokens\": min(\n                    body.get(\"max_tokens\", max_tokens_limit), max_tokens_limit\n                ),\n                \"temperature\": float(body.get(\"temperature\", 0.7)),\n                \"stream\": body.get(\"stream\", False),\n            }\n\n            headers = {\n                \"Authorization\": f\"Bearer {self.valves.DEEPSEEK_API_KEY}\",\n                \"Content-Type\": \"application/json\",\n            }\n\n            if payload[\"stream\"]:\n                return self._stream_response(\n                    url=f\"{self.valves.DEEPSEEK_BASE_URL}/chat/completions\",\n                    headers=headers,\n                    payload=payload,\n                    __event_emitter__=__event_emitter__,\n                    model_id=model_id,\n                )\n            else:\n                response_data = await self._regular_request(\n                    url=f\"{self.valves.DEEPSEEK_BASE_URL}/chat/completions\",\n                    headers=headers,\n                    payload=payload,\n                    __event_emitter__=__event_emitter__,\n                    model_id=model_id,\n                )\n\n                # Ensure response structure consistency\n                if \"error\" in response_data:\n                    return response_data\n                if not response_data.get(\"choices\"):\n                    return {\"error\": \"Empty response from API\", \"choices\": []}\n                return response_data\n\n        except Exception as e:\n            error_msg = f\"Error: {str(e)}\"\n            logging.error(f\"Pipe processing failed: {error_msg}\")\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\"type\": \"status\", \"data\": {\"description\": error_msg, \"done\": True}}\n                )\n            return {\"error\": error_msg, \"choices\": []}\n","meta":{"description":"DeepSeek Vision","manifest":{"title":"Deepseek R1 Manifold Pipe with Gemini Vision Support","authors":"[MCode-Team, Ethan Copping, zgccrui]","author_url":"[https://github.com/MCode-Team, https://github.com/CoppingEthan]","funding_url":"https://github.com/open-webui","version":"0.1.7","license":"MIT","environment_variables":"","User":"[Text + Image]","System":""}},"is_active":false,"is_global":false,"updated_at":1741624355,"created_at":1741618834}]